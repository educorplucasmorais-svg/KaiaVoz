# GitHub Copilot Quick Start - NeuralCore Integration

## ğŸ“‚ Arquivos NeuralCore - LocalizaÃ§Ã£o RÃ¡pida

### **Arquivos Principais (Production)**
- ğŸ”¹ [apps/server/src/services/neuralcore.ts](apps/server/src/services/neuralcore.ts) - **NeuralCoreAgent** principal
- ğŸ”¹ [apps/server/src/routes/ai.ts](apps/server/src/routes/ai.ts) - **API endpoints** `/api/ai/voice` e `/api/ai/process`
- ğŸ”¹ [apps/frontend/src/hooks/useNeuralCore.ts](apps/frontend/src/hooks/useNeuralCore.ts) - **React hook** para integraÃ§Ã£o

### **Arquivos Alternativos (server-kaia)**
- ğŸ”¹ [apps/server-kaia/src/NeuralCoreAgent.ts](apps/server-kaia/src/NeuralCoreAgent.ts) - VersÃ£o alternativa com tipos
- ğŸ”¹ [apps/server-kaia/src/server-integration.ts](apps/server-kaia/src/server-integration.ts) - IntegraÃ§Ã£o de servidor

### **Frontend Integration**
- ğŸ”¹ [apps/frontend/src/App.tsx](apps/frontend/src/App.tsx) - App principal integrado
- ğŸ”¹ [apps/frontend/src/hooks/useSpeech.ts](apps/frontend/src/hooks/useSpeech.ts) - STT com permission automÃ¡tica

## ğŸš€ Quick Commands

```bash
# Iniciar desenvolvimento
npm install
npm run dev

# Rodar frontend (porta 5173)
npm run dev --workspace=@kaia/frontend

# Rodar backend (porta 3060)
npm run dev --workspace=@kaia/server

# Fazer build
npm run build
```

## ğŸ“Š Fluxo de Dados

```
[Frontend Speech Input]
    â†“
[useNeuralCore Hook]
    â†“
POST /api/ai/voice
    â†“
[NeuralCoreAgent.processVoiceCommand()]
    â†“
[Analyze Intent] â†’ {greeting|reminder|command|question|system_control|conversation}
    â†“
[Generate Response] â†’ {thought, speak, toolCall?}
    â†“
[Speaker Output via ElevenLabs TTS]
```

## ğŸ¯ 6 Intent Types

| Intent | Exemplo | Handler |
|--------|---------|---------|
| **greeting** | "OlÃ¡", "Como vai?" | `handleGreeting()` |
| **reminder** | "Lembrar-me de... Ã s 3pm" | `handleReminder()` |
| **command** | "Abrir arquivo.txt", "Aumentar volume" | `handleFileOperation()`, `handleSystemControl()` |
| **question** | "Qual Ã© a data de hoje?" | `handleQuestion()` |
| **system_control** | "Pausar mÃºsica", "Mute" | `handleSystemControl()` |
| **conversation** | Texto livre | Default response |

## ğŸ” Security Features

- **Regex-based blocker** para patterns perigosos (system32, delete, shutdown, etc.)
- **ValidaÃ§Ã£o antes de processar** qualquer comando
- **ConfirmaÃ§Ã£o necessÃ¡ria** para operaÃ§Ãµes de arquivo

## ğŸ“ Response Format

```typescript
interface NeuralCoreResponse {
  thought: string;      // AnÃ¡lise do intent
  speak: string;        // O que o sistema fala
  toolCall?: {
    type: 'reminder' | 'file_operation' | 'system_control';
    action: string;
    data?: any;
  };
}
```

## ğŸ”§ ConfiguraÃ§Ã£o

### ElevenLabs TTS
```bash
# Copiar .env.example para .env
cp apps/server/.env.example apps/server/.env

# Adicionar suas credenciais
ELEVEN_LABS_API_KEY=sk_xxxxx
ELEVEN_LABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM  # Rachel
```

### Web Speech API
- AutomÃ¡tico no navegador (portuguÃªs pt-BR)
- Funciona offline
- Fallback para sÃ­ntese do navegador se ElevenLabs falhar

## ğŸ“š DocumentaÃ§Ã£o Completa

Veja [NEURALCORE_GUIDE.md](NEURALCORE_GUIDE.md) para:
- Arquitetura completa
- Guia de desenvolvimento
- API reference completa
- Exemplos de extensÃ£o

## âš¡ Dicas para GitHub Copilot

### Para adicionar novo intent type:

1. Edite `apps/server/src/services/neuralcore.ts`
2. Adicione padrÃ£o em `analyzeIntent()`:
```typescript
if (/seu_padrÃ£o/i.test(text)) return 'seu_tipo';
```

3. Adicione handler em `generateResponse()`:
```typescript
case 'seu_tipo':
  return this.handleSeuTipo(text);
```

4. Implemente o handler:
```typescript
private handleSeuTipo(text: string): NeuralCoreResponse {
  // Sua lÃ³gica aqui
}
```

### Para adicionar nova rota API:

1. Crie arquivo em `apps/server/src/routes/novo.ts`
2. Importe em `apps/server/src/index.ts`
3. Registre: `app.use('/api/novo', novoRouter)`

## ğŸ› Troubleshooting

**Copilot nÃ£o encontra arquivo?**
â†’ Execute `git add -A && git commit && git push`

**Port jÃ¡ em uso?**
â†’ `npm run dev` automÃ¡ticamente usa prÃ³xima porta disponÃ­vel

**Microphone nÃ£o pedindo permissÃ£o?**
â†’ Verifique `apps/frontend/src/hooks/useSpeech.ts` - `requestMicPermission()` deve ser chamado em `useEffect`

**Backend nÃ£o responde?**
â†’ Verifique proxy em `apps/frontend/vite.config.ts` - deve apontar para `http://localhost:3060`

## ğŸ“ API Examples

### Request
```bash
curl -X POST http://localhost:3060/api/ai/voice \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Lembrar-me de comprar leite Ã s 3 da tarde",
    "userId": "user123"
  }'
```

### Response
```json
{
  "thought": "User wants to set a reminder",
  "speak": "Anotei. Vou lembrar vocÃª de comprar leite Ã s 3 da tarde",
  "toolCall": {
    "type": "reminder",
    "action": "create",
    "data": {
      "text": "comprar leite",
      "time": "15:00"
    }
  }
}
```

---

**Ãšltima atualizaÃ§Ã£o**: Dezembro 2024
**Status**: âœ… Production-Ready
**GitHub**: https://github.com/educorplucasmorais-svg/KaiaVoz
